{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b1d376",
   "metadata": {},
   "source": [
    "# **00 Import Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0fb126fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformersNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "     -------------------------------------- 470.2/470.2 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting transformers<5.0.0,>=4.41.0\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "     ---------------------------------------- 10.8/10.8 MB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.0.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.1.1)\n",
      "Collecting huggingface-hub>=0.20.0\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "     -------------------------------------- 515.3/515.3 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sentence-transformers) (9.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.11.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2.8.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.24.4)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 2.4 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "     -------------------------------------- 308.9/308.9 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.20)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.2.1)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.0\n",
      "    Uninstalling safetensors-0.4.0:\n",
      "      Successfully uninstalled safetensors-0.4.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.19.4\n",
      "    Uninstalling huggingface-hub-0.19.4:\n",
      "      Successfully uninstalled huggingface-hub-0.19.4\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "Successfully installed huggingface-hub-0.33.4 safetensors-0.5.3 sentence-transformers-5.0.0 tokenizers-0.21.2 transformers-4.53.2\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af81e663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\hp\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Text Vectorization\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Modeling\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.similarities import WmdSimilarity\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158b95d",
   "metadata": {},
   "source": [
    "# **01 Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde3340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job = pd.read_csv(\"cleaned_jobstreet.csv\")\n",
    "df_courses = pd.read_csv(\"cleaned_classentral.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8816e5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 480 entries, 0 to 479\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   url          480 non-null    object\n",
      " 1   country      480 non-null    object\n",
      " 2   title        480 non-null    object\n",
      " 3   company      480 non-null    object\n",
      " 4   location     480 non-null    object\n",
      " 5   category     480 non-null    object\n",
      " 6   work_type    480 non-null    object\n",
      " 7   description  480 non-null    object\n",
      " 8   text         480 non-null    object\n",
      " 9   text_clean   480 non-null    object\n",
      " 10  tokens       480 non-null    object\n",
      "dtypes: object(11)\n",
      "memory usage: 41.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_job.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f749294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   title           10 non-null     object \n",
      " 1   provider        10 non-null     object \n",
      " 2   language        10 non-null     object \n",
      " 3   certificate     10 non-null     object \n",
      " 4   average rating  10 non-null     float64\n",
      " 5   price type      10 non-null     object \n",
      " 6   reviews         10 non-null     object \n",
      " 7   overview        10 non-null     object \n",
      " 8   skills          10 non-null     object \n",
      " 9   text            10 non-null     object \n",
      " 10  text_clean      10 non-null     object \n",
      " 11  tokens          10 non-null     object \n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 1.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_courses.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91a7fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gabungkan semua token jadi satu list untuk Word2Vec\n",
    "all_sentences = df_job[\"tokens\"].tolist() + df_courses[\"tokens\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e415a",
   "metadata": {},
   "source": [
    "# **02 Text Vectorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec451731",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3049a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(sentences=all_sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "def sentence_vector(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "df_job[\"w2v_vec\"] = df_job[\"tokens\"].apply(lambda x: sentence_vector(x, w2v_model))\n",
    "df_courses[\"w2v_vec\"] = df_courses[\"tokens\"].apply(lambda x: sentence_vector(x, w2v_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c6a1a",
   "metadata": {},
   "source": [
    "## Sentence Tranformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7c044b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_st = SentenceTransformer('all-MiniLM-L6-v2')  # ringan & cepat\n",
    "\n",
    "# # Encode kolom 'text_clean'\n",
    "# df_job[\"st_vec\"] = list(model_st.encode(df_job[\"text_clean\"].tolist(), show_progress_bar=True))\n",
    "# df_courses[\"st_vec\"] = list(model_st.encode(df_courses[\"text_clean\"].tolist(), show_progress_bar=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2cd13",
   "metadata": {},
   "source": [
    "# **03 Labelling & Similarity Mapping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dfdf13",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0631a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung vektor rata-rata Word2Vec\n",
    "def sentence_vector(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# Buat vektor rata-rata\n",
    "df_job[\"w2v_vec\"] = df_job[\"tokens\"].apply(lambda x: sentence_vector(x, w2v_model))\n",
    "df_courses[\"w2v_vec\"] = df_courses[\"tokens\"].apply(lambda x: sentence_vector(x, w2v_model))\n",
    "\n",
    "# Fungsi mapping job â†’ course via Cosine\n",
    "def map_job_to_courses_cosine(df_jobs, df_courses, top_k=3):\n",
    "    course_matrix = np.stack(df_courses[\"w2v_vec\"].values)\n",
    "    results = []\n",
    "\n",
    "    for idx, row in df_jobs.iterrows():\n",
    "        job_vec = row[\"w2v_vec\"].reshape(1, -1)\n",
    "        sims = cosine_similarity(job_vec, course_matrix).flatten()\n",
    "        top_indices = sims.argsort()[-top_k:][::-1]\n",
    "\n",
    "        matches = [{\n",
    "            \"course_title\": df_courses.iloc[i][\"title\"],\n",
    "            \"score\": round(sims[i], 4)\n",
    "        } for i in top_indices]\n",
    "\n",
    "        results.append({\n",
    "            \"job_title\": row[\"title\"],\n",
    "            \"job_category\": row[\"category\"],\n",
    "            \"recommended_courses\": matches\n",
    "        })\n",
    "    return results\n",
    "\n",
    "job_course_sim_cosine = map_job_to_courses_cosine(df_job, df_courses, top_k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7ea15",
   "metadata": {},
   "source": [
    "## WMD Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b959c94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi indeks WMD dari course\n",
    "wmd_index = WmdSimilarity(df_courses[\"tokens\"].tolist(), w2v_model.wv, num_best=3)\n",
    "\n",
    "# Mapping job â†’ course via WMD\n",
    "def map_job_to_courses_wmd(df_jobs, df_courses, wmd_index, top_k=3):\n",
    "    results = []\n",
    "\n",
    "    for idx, row in df_jobs.iterrows():\n",
    "        sims = wmd_index[row[\"tokens\"]][:top_k]\n",
    "\n",
    "        matches = [{\n",
    "            \"course_title\": df_courses.iloc[i][\"title\"],\n",
    "            \"score\": round(score, 4)\n",
    "        } for i, score in sims]\n",
    "\n",
    "        results.append({\n",
    "            \"job_title\": row[\"title\"],\n",
    "            \"job_category\": row[\"category\"],\n",
    "            \"recommended_courses\": matches\n",
    "        })\n",
    "    return results\n",
    "\n",
    "job_course_sim_wmd = map_job_to_courses_wmd(df_job, df_courses, wmd_index, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aac6b9",
   "metadata": {},
   "source": [
    "# 04 Recommendation Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4cc61e",
   "metadata": {},
   "source": [
    "### b. Cosine + W2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0567ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# COSINE SIMILARITY MODELING\n",
    "# ============================\n",
    "def recommend_courses_cosine(df_jobs, df_courses, top_k=3):\n",
    "    course_matrix = np.stack(df_courses[\"w2v_vec\"].values)\n",
    "    recommendations = []\n",
    "\n",
    "    for idx, row in df_jobs.iterrows():\n",
    "        job_vec = row[\"w2v_vec\"].reshape(1, -1)\n",
    "        sims = cosine_similarity(job_vec, course_matrix).flatten()\n",
    "        top_indices = sims.argsort()[-top_k:][::-1]\n",
    "\n",
    "        matches = [{\n",
    "            \"course_title\": df_courses.iloc[i][\"title\"],\n",
    "            \"score\": round(sims[i], 4)\n",
    "        } for i in top_indices]\n",
    "\n",
    "        recommendations.append({\n",
    "            \"job\": row[\"title\"],\n",
    "            \"category\": row[\"category\"],\n",
    "            \"matches\": matches\n",
    "        })\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e075b5a",
   "metadata": {},
   "source": [
    "### e. WMD + W2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e73640be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# WMD SIMILARITY MODELING\n",
    "# ============================\n",
    "def recommend_courses_wmd(df_jobs, df_courses, wmd_index, top_k=3):\n",
    "    recommendations = []\n",
    "\n",
    "    for idx, row in df_jobs.iterrows():\n",
    "        sims = wmd_index[row[\"tokens\"]][:top_k]\n",
    "\n",
    "        matches = [{\n",
    "            \"course_title\": df_courses.iloc[i][\"title\"],\n",
    "            \"score\": round(score, 4)\n",
    "        } for i, score in sims]\n",
    "\n",
    "        recommendations.append({\n",
    "            \"job\": row[\"title\"],\n",
    "            \"category\": row[\"category\"],\n",
    "            \"matches\": matches\n",
    "        })\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f5b208ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cosine + Word2Vec] Job: data scientist, financial conglomerates supervision\n",
      "Category: analysis & reporting (banking & financial services)\n",
      "Rekomendasi Course:\n",
      "   - cs50's computer science for lawyers                | Score: 0.9880\n",
      "   - cs50's computer science for business professionals | Score: 0.9874\n",
      "   - introduction to electrical engineering and computer science i | Score: 0.9833\n",
      "   - cs50's introduction to computer science            | Score: 0.9829\n",
      "   - introduction to computer science and programming using python | Score: 0.9765\n",
      "\n",
      "[Cosine + Word2Vec] Job: data scientist\n",
      "Category: mathematics, statistics & information sciences (science & technology)\n",
      "Rekomendasi Course:\n",
      "   - cs50's computer science for lawyers                | Score: 0.9913\n",
      "   - cs50's introduction to computer science            | Score: 0.9894\n",
      "   - harvard cs50 â€“ full computer science university course | Score: 0.9851\n",
      "   - cs50's computer science for business professionals | Score: 0.9851\n",
      "   - introduction to electrical engineering and computer science i | Score: 0.9840\n",
      "\n",
      "[WMD + Word2Vec] Job: data scientist, financial conglomerates supervision\n",
      "Category: analysis & reporting (banking & financial services)\n",
      "Rekomendasi Course:\n",
      "   - cs50's computer science for business professionals | Score: 0.8798\n",
      "   - computational social science methods               | Score: 0.8662\n",
      "   - introduction to electrical engineering and computer science i | Score: 0.8651\n",
      "\n",
      "[WMD + Word2Vec] Job: data scientist\n",
      "Category: mathematics, statistics & information sciences (science & technology)\n",
      "Rekomendasi Course:\n",
      "   - cs50's computer science for business professionals | Score: 0.8787\n",
      "   - harvard cs50 â€“ full computer science university course | Score: 0.8787\n",
      "   - cs50's computer science for lawyers                | Score: 0.8749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_recommendation_results(results, method=\"Cosine + W2V\"):\n",
    "    for res in results:\n",
    "        print(f\"[{method}] Job: {res['job']}\")\n",
    "        print(f\"Category: {res['category']}\")\n",
    "        print(\"Rekomendasi Course:\")\n",
    "        for match in res['matches']:\n",
    "            print(f\"   - {match['course_title']:<50} | Score: {match['score']:.4f}\")\n",
    "        print()\n",
    "        \n",
    "# 1. Cosine\n",
    "results_cosine = recommend_courses_cosine(df_job, df_courses, top_k=5)\n",
    "print_recommendation_results(results_cosine[:2], method=\"Cosine + Word2Vec\")\n",
    "\n",
    "# 2. WMD\n",
    "from gensim.similarities import WmdSimilarity\n",
    "wmd_index = WmdSimilarity(df_courses[\"tokens\"].tolist(), w2v_model.wv, num_best=3)\n",
    "\n",
    "results_wmd = recommend_courses_wmd(df_job, df_courses, wmd_index, top_k=3)\n",
    "print_recommendation_results(results_wmd[:2], method=\"WMD + Word2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d202573",
   "metadata": {},
   "source": [
    "### Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f280e3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¼ [COSINE] User Query: Data Analyst\n",
      "ðŸ“š Recommended Courses:\n",
      "   - cs50's computer science for lawyers | Score: 0.0\n",
      "   - functional programming principles in scala | Score: 0.0\n",
      "   - computational social science methods | Score: 0.0\n",
      "\n",
      "\n",
      "ðŸ’¼ [WMD] User Query: Data Analyst\n",
      "ðŸ“š Recommended Courses:\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"data scientist skilled in Python and machine learning\"\n",
    "\n",
    "# Tokenisasi ringan (tanpa NLTK)\n",
    "def basic_tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Representasi Word2Vec rata-rata\n",
    "def sentence_vector(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "def recommend_courses_from_text_cosine(user_text, df_courses, model, top_k=3):\n",
    "    tokens = basic_tokenize(user_text)\n",
    "    user_vec = sentence_vector(tokens, model).reshape(1, -1)\n",
    "\n",
    "    course_matrix = np.stack(df_courses[\"w2v_vec\"].values)\n",
    "    similarities = cosine_similarity(user_vec, course_matrix).flatten()\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "\n",
    "    return [{\n",
    "        \"course_title\": df_courses.iloc[i][\"title\"],\n",
    "        \"score\": round(similarities[i], 4)\n",
    "    } for i in top_indices]\n",
    "\n",
    "def recommend_courses_from_text_wmd(user_text, wmd_index, top_k=3):\n",
    "    tokens = basic_tokenize(user_text)\n",
    "    sims = wmd_index[tokens][:top_k]\n",
    "\n",
    "    return [{\n",
    "        \"course_title\": df_courses.iloc[i][\"title\"],\n",
    "        \"score\": round(score, 4)\n",
    "    } for i, score in sims]\n",
    "\n",
    "# Sample user input\n",
    "sample_text = \"data scientist skilled in Python and machine learning\"\n",
    "\n",
    "# COSINE RECOMMENDATION\n",
    "recommendations_cosine = recommend_courses_from_text_cosine(sample_text, df_courses, w2v_model, top_k=3)\n",
    "\n",
    "# WMD RECOMMENDATION (make sure wmd_index sudah dibuat)\n",
    "wmd_index = WmdSimilarity(df_courses[\"tokens\"].tolist(), w2v_model.wv, num_best=3)\n",
    "recommendations_wmd = recommend_courses_from_text_wmd(sample_text, wmd_index, top_k=3)\n",
    "\n",
    "# Output: Cosine\n",
    "print(\"ðŸ’¼ [COSINE] User Query: Data Analyst\")\n",
    "print(\"ðŸ“š Recommended Courses:\")\n",
    "for rec in recommendations_cosine:\n",
    "    print(f\"   - {rec['course_title']} | Score: {rec['score']}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Output: WMD\n",
    "print(\"ðŸ’¼ [WMD] User Query: Data Analyst\")\n",
    "print(\"ðŸ“š Recommended Courses:\")\n",
    "for rec in recommendations_wmd:\n",
    "    print(f\"   - {rec['course_title']} | Score: {rec['score']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad6b212",
   "metadata": {},
   "source": [
    "## Saving Vectorization & Similarity Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561db3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15268bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9262bfdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

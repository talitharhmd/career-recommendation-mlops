{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d4e0cbe",
   "metadata": {},
   "source": [
    "\n",
    "# **Web Scraping Class Central dengan Selenium**\n",
    "---\n",
    "Pada bagian ini, akan dilakukan web scraping dari situs [Class Central](https://www.classcentral.com/subject/cs) menggunakan **Selenium**.\n",
    "\n",
    "Tujuan scraping ini adalah untuk mengambil data kursus seperti judul, provider, rating, bahasa, ketersediaan sertifikat, dan status gratis/berbayar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94c2a920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries installed successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "print(\"All libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e30cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver():\n",
    "\n",
    "  # Fungsi untuk menyiapkan dan mengembalikan WebDriver (Chrome) di Google Colab.\n",
    "  # Menggunakan opsi headless dan beberapa flags tambahan agar dapat berjalan stabil.\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Hapus jika ingin tampilkan browser\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9974eb8",
   "metadata": {},
   "source": [
    "## Scraping Data Class Central\n",
    "\n",
    "Mengambil data dari 10 halaman pertama pada kategori Computer Science di Class Central. Untuk setiap kursus, kita ambil informasi:\n",
    "- Judul kursus\n",
    "- Provider/platform\n",
    "- Bahasa\n",
    "- Sertifikat (tersedia/tidak)\n",
    "- Rating rata-rata\n",
    "- Status (gratis atau tidak)\n",
    "- Jumlah ulasan\n",
    "- Link ke kursus\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Menggunakan struktur HTML dan atribut `data-track-props` untuk mengekstrak informasi tersebut.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22990135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_classcentral(driver, urls_to_scrape):\n",
    "\n",
    "    # Melakukan scraping dari halaman-halaman Class Central berdasarkan URL dasar.\n",
    "\n",
    "    all_courses_data = []\n",
    "    for subject, base_url in urls_to_scrape.items():\n",
    "      logging.info(f\"===== Memulai scraping untuk subjek: {subject.upper()} =====\")\n",
    "\n",
    "      for page in range(1, 101):  # Halaman 1 sampai 200\n",
    "            url = f\"{base_url}?page={page}\"\n",
    "            logging.info(f\"Scraping halaman {page}: {url}\")\n",
    "            driver.get(url)\n",
    "            time.sleep(3)  # Jeda untuk memberi waktu halaman loading\n",
    "\n",
    "            # Cari semua elemen yang merupakan nama kursus\n",
    "            course_cards = driver.find_elements(By.CSS_SELECTOR, 'li.course-list-course')\n",
    "            logging.info(f\"Halaman {page}: ditemukan {len(course_cards)} kursus.\")\n",
    "\n",
    "            for card in course_cards:\n",
    "                try:\n",
    "                    # Cari judul di dalam 'kartu' kursus\n",
    "                    title_elem = card.find_element(By.CSS_SELECTOR, 'a.color-charcoal.course-name')\n",
    "                    # Ambil teks judul dan link\n",
    "                    title = title_elem.text.strip()\n",
    "                    link = title_elem.get_attribute('href')\n",
    "\n",
    "                    # Ambil atribut JSON tersembunyi untuk informasi tambahan\n",
    "                    data_props_raw = title_elem.get_attribute('data-track-props')\n",
    "                    data_props = json.loads(data_props_raw)\n",
    "\n",
    "                    provider = data_props.get(\"course_provider\", \"Unknown\")\n",
    "\n",
    "                    # Certificate & Price Formatting\n",
    "                    certificate = \"Certificate Available\" if data_props.get(\"course_certificate\", False) else \"No Certificate\"\n",
    "\n",
    "                    language = data_props.get(\"course_language\", \"N/A\")\n",
    "                    avg_rating = round(data_props.get(\"course_avg_rating\", 0.0), 1)\n",
    "\n",
    "                    is_free = data_props.get(\"course_is_free\", False)\n",
    "                    price_type = \"Free\" if is_free else \"Paid\"\n",
    "\n",
    "                    overview_elem = card.find_element(By.CSS_SELECTOR, 'a.color-charcoal.block.hover-no-underline.break-word')\n",
    "                    course_overview = overview_elem.text.strip() if overview_elem else \"\"\n",
    "\n",
    "                    # Ambil container untuk durasi (parent element dua tingkat ke atas)\n",
    "                    try:\n",
    "                        # Kita cari elemen durasi di dalam 'card' yang sudah kita temukan\n",
    "                        duration_elem = card.find_element(By.CSS_SELECTOR, 'span[aria-label=\"Workload and duration\"]')\n",
    "                        duration = duration_elem.text.strip()\n",
    "                    except:\n",
    "                        duration = \"N/A\" # Fallback jika tidak ditemukan\n",
    "\n",
    "                    try:\n",
    "                        # Coba ambil teks ulasan\n",
    "                        reviews = title_elem.find_element(By.XPATH, '../..').find_element(By.CSS_SELECTOR, 'span.color-gray').text.strip()\n",
    "                    except:\n",
    "                        reviews = \"0 reviews\"\n",
    "\n",
    "                    # Simpan data ke list\n",
    "                    all_courses_data.append({\n",
    "                        'title': title,\n",
    "                        'category': subject,\n",
    "                        'provider': provider,\n",
    "                        'language': language,\n",
    "                        'certificate': certificate,\n",
    "                        'avg_rating': avg_rating,\n",
    "                        'price_type': price_type,\n",
    "                        'reviews': reviews,\n",
    "                        'duration': duration,\n",
    "                        'overview': course_overview,\n",
    "                        'link': f\"https://www.classcentral.com{link}\"\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error saat membaca 1 kursus di halaman {page}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    # Konversi hasil scraping ke DataFrame\n",
    "    return pd.DataFrame(all_courses_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b297b305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Scraping tidak menghasilkan data.\n"
     ]
    }
   ],
   "source": [
    "# Inisialisasi WebDriver\n",
    "driver = None\n",
    "try:\n",
    "    driver = setup_driver()\n",
    "\n",
    "    target_urls = {\n",
    "        \"Data Science\": \"https://www.classcentral.com/subject/data-science\",\n",
    "        \"Machine Learning\": \"https://www.classcentral.com/subject/machine-learning\",\n",
    "        \"Data Analysis\": \"https://www.classcentral.com/subject/data-analysis\",\n",
    "        \"Data Engineering\": \"https://www.classcentral.com/subject/data-engineering\",\n",
    "        \"Computer Science\": \"https://www.classcentral.com/subject/cs\"\n",
    "    }\n",
    "\n",
    "    scraped_data = scrape_classcentral(driver, target_urls)\n",
    "\n",
    "    if not scraped_data.empty:\n",
    "        scraped_data.drop_duplicates(subset=['title', 'provider'], inplace=True, keep='first')\n",
    "\n",
    "        new_column_names = {\n",
    "            'title': 'Title',\n",
    "            'category': 'Category',\n",
    "            'provider': 'Provider',\n",
    "            'language': 'Language',\n",
    "            'certificate': 'Certificate',\n",
    "            'avg_rating': 'Average Rating',\n",
    "            'price_type': 'Price Type',\n",
    "            'reviews': 'Reviews',\n",
    "            'duration': 'Duration',\n",
    "            'overview': 'Overview',\n",
    "            'link': 'Link'\n",
    "        }\n",
    "\n",
    "        scraped_data.rename(columns=new_column_names, inplace=True)\n",
    "        logging.info(\"Nama kolom berhasil diubah.\")\n",
    "\n",
    "        # Simpan ke CSV dengan nama kolom yang sudah baru\n",
    "        output_file_csv = 'courses_data.csv'\n",
    "        scraped_data.to_csv(output_file_csv, index=False)\n",
    "\n",
    "        print(f\"\\n‚úÖ Scraping selesai. Data disimpan ke: '{output_file_csv}'\")\n",
    "        print(\"Contoh 5 baris pertama data dengan header baru:\")\n",
    "        print(scraped_data.head())\n",
    "    else:\n",
    "        logging.warning(\"Scraping tidak menghasilkan data.\")\n",
    "\n",
    "finally:\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "        logging.info(\"WebDriver ditutup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b35ac13b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'courses_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37892\\3707865667.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'courses_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'courses_data.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('courses_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5596a94",
   "metadata": {},
   "source": [
    "## Scraping Data JobStreet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb580d",
   "metadata": {},
   "source": [
    "Mengambil data dari 10 halaman pertama pada kategori Computer Science di Class Central. Untuk setiap kursus, kita ambil informasi:\n",
    "- Judul kursus\n",
    "- Provider/platform\n",
    "- Bahasa\n",
    "- Sertifikat (tersedia/tidak)\n",
    "- Rating rata-rata\n",
    "- Status (gratis atau tidak)\n",
    "- Jumlah ulasan\n",
    "- Link ke kursus\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Menggunakan struktur HTML dan atribut `data-track-props` untuk mengekstrak informasi tersebut.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4fb762",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"data scientist\", \"data analyst\", \"machine learning\", \"data engineer\", \"data science\", \"computer science\"]\n",
    "\n",
    "# Function for scraping per country\n",
    "def scrape_jobstreet(domain, country_label):\n",
    "    all_hrefs = []\n",
    "    for keyword in keywords:\n",
    "        search_keyword = keyword.replace(\" \", \"-\")\n",
    "        search_url = f\"https://{domain}/en/job-search/{search_keyword}-jobs/\"\n",
    "        print(f\"\\nSearching on {country_label.upper()} for: {keyword.upper()} jobs\")\n",
    "        driver.get(search_url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"a[data-automation='jobTitle']\"))\n",
    "            )\n",
    "            job_links_elements = driver.find_elements(By.CSS_SELECTOR, \"a[data-automation='jobTitle']\")\n",
    "            hrefs = [(el.get_attribute(\"href\"), country_label) for el in job_links_elements if el.get_attribute(\"href\")]\n",
    "            all_hrefs.extend(hrefs)\n",
    "            print(f\"  Found {len(hrefs)} job links in {country_label} for '{keyword}'\")\n",
    "        except TimeoutException:\n",
    "            print(f\"  Timeout loading results for: {keyword} in {country_label}\")\n",
    "    return all_hrefs\n",
    "\n",
    "# Scrape per country\n",
    "all_job_hrefs_malaysia = scrape_jobstreet(\"jobstreet.com.my\", \"malaysia\")\n",
    "all_job_hrefs_singapore = scrape_jobstreet(\"jobstreet.com.sg\", \"singapore\")\n",
    "all_job_hrefs_indonesia = scrape_jobstreet(\"id.jobstreet.com\", \"indonesia\")\n",
    "\n",
    "# Gabungkan semua\n",
    "all_job_hrefs = all_job_hrefs_malaysia + all_job_hrefs_singapore + all_job_hrefs_indonesia\n",
    "\n",
    "# Deduplicate\n",
    "unique_href_map = {}\n",
    "for href, country in all_job_hrefs:\n",
    "    if href and href not in unique_href_map:\n",
    "        unique_href_map[href] = country\n",
    "\n",
    "# Scrape detail tiap job\n",
    "scraped_jobs = []\n",
    "for i, (href, country_name) in enumerate(list(unique_href_map.items())):\n",
    "    if not href.startswith(\"http\"):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n[{i+1}] Navigating to: {href}\")\n",
    "    try:\n",
    "        driver.get(href)\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"h1[data-automation='job-detail-title']\"))\n",
    "        )\n",
    "\n",
    "        title = driver.find_element(By.CSS_SELECTOR, \"h1[data-automation='job-detail-title']\").text\n",
    "        company = driver.find_element(By.CSS_SELECTOR, \"span[data-automation='advertiser-name']\").text\n",
    "        location = driver.find_element(By.CSS_SELECTOR, \"span[data-automation='job-detail-location']\").text\n",
    "        category = driver.find_element(By.CSS_SELECTOR, \"span[data-automation='job-detail-classifications']\").text\n",
    "        work_type = driver.find_element(By.CSS_SELECTOR, \"span[data-automation='job-detail-work-type']\").text\n",
    "        description = driver.find_element(By.CSS_SELECTOR, \"div[data-automation='jobAdDetails']\").text\n",
    "\n",
    "        # Tambahan: Gaji\n",
    "        try:\n",
    "            salary_element = driver.find_element(By.CSS_SELECTOR, \"span[data-automation='job-detail-salary']\")\n",
    "            salary = salary_element.text\n",
    "        except:\n",
    "            salary = \"Not specified\"\n",
    "\n",
    "        # Tambahan: Ekstrak requirement sederhana dari deskripsi\n",
    "        requirements = \"\"\n",
    "        for line in description.splitlines():\n",
    "            if \"requirement\" in line.lower() or \"qualification\" in line.lower() or \"kualifikasi\" in line.lower():\n",
    "                requirements += line.strip() + \" | \"\n",
    "        if not requirements:\n",
    "            requirements = \"Not specified\"\n",
    "\n",
    "        scraped_jobs.append({\n",
    "            \"Title\": title.strip(),\n",
    "            \"Company\": company.strip(),\n",
    "            \"Country\": country_name.strip().title(),\n",
    "            \"Location\": location.strip(),\n",
    "            \"Category\": category.strip(),\n",
    "            \"Work Type\": work_type.strip(),\n",
    "            \"Salary\": salary.strip(),\n",
    "            \"Requirements\": requirements.strip(),\n",
    "            \"Description\": description.strip(),\n",
    "            \"Link\": href.strip()\n",
    "        })\n",
    "\n",
    "        print(f\"  Title: {title}\")\n",
    "        print(f\"  Company: {company}\")\n",
    "        print(f\"  Country: {country_name}\")\n",
    "        print(f\"  Location: {location}\")\n",
    "        print(f\"  Category: {category}\")\n",
    "        print(f\"  Work Type: {work_type}\")\n",
    "        print(f\"  Salary: {salary}\")\n",
    "        print(f\"  Requirements: {requirements}\")\n",
    "        print(f\"  Description preview: {description[:200]}...\")\n",
    "        print(f\"  Link: {href}\")\n",
    "\n",
    "    except (TimeoutException, NoSuchElementException) as e:\n",
    "        print(f\"  Error scraping {href}: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "df_jobs = pd.DataFrame(scraped_jobs)\n",
    "print(\"\\nTotal jobs scraped:\", len(df_jobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b45edb",
   "metadata": {},
   "source": [
    "# Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a81a269b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching on MALAYSIA for: DATA SCIENTIST\n",
      "  Timeout loading results for: data scientist in malaysia\n",
      "\n",
      "Searching on MALAYSIA for: DATA ANALYST\n",
      "  Timeout loading results for: data analyst in malaysia\n",
      "\n",
      "Searching on MALAYSIA for: MACHINE LEARNING\n",
      "  Timeout loading results for: machine learning in malaysia\n",
      "\n",
      "Searching on MALAYSIA for: DATA ENGINEER\n",
      "  Timeout loading results for: data engineer in malaysia\n",
      "\n",
      "Searching on MALAYSIA for: DATA SCIENCE\n",
      "  Timeout loading results for: data science in malaysia\n",
      "\n",
      "Searching on SINGAPORE for: DATA SCIENTIST\n",
      "  Timeout loading results for: data scientist in singapore\n",
      "\n",
      "Searching on SINGAPORE for: DATA ANALYST\n",
      "  Timeout loading results for: data analyst in singapore\n",
      "\n",
      "Searching on SINGAPORE for: MACHINE LEARNING\n",
      "  Timeout loading results for: machine learning in singapore\n",
      "\n",
      "Searching on SINGAPORE for: DATA ENGINEER\n",
      "  Timeout loading results for: data engineer in singapore\n",
      "\n",
      "Searching on SINGAPORE for: DATA SCIENCE\n",
      "  Timeout loading results for: data science in singapore\n",
      "\n",
      "Searching on INDONESIA for: DATA SCIENTIST\n",
      "  Timeout loading results for: data scientist in indonesia\n",
      "\n",
      "Searching on INDONESIA for: DATA ANALYST\n",
      "  Timeout loading results for: data analyst in indonesia\n",
      "\n",
      "Searching on INDONESIA for: MACHINE LEARNING\n",
      "  Timeout loading results for: machine learning in indonesia\n",
      "\n",
      "Searching on INDONESIA for: DATA ENGINEER\n",
      "  Timeout loading results for: data engineer in indonesia\n",
      "\n",
      "Searching on INDONESIA for: DATA SCIENCE\n",
      "  Timeout loading results for: data science in indonesia\n",
      "\n",
      "‚úÖ JobStreet scraping done. Saved to cleaned_jobstreet.csv\n",
      "\n",
      "üìö Scraping Subject: Data Science\n",
      "  Page 1: 0 courses found.\n",
      "  Page 2: 0 courses found.\n",
      "  Page 3: 0 courses found.\n",
      "  Page 4: 0 courses found.\n",
      "  Page 5: 0 courses found.\n",
      "\n",
      "üìö Scraping Subject: Machine Learning\n",
      "  Page 1: 0 courses found.\n",
      "  Page 2: 0 courses found.\n",
      "  Page 3: 0 courses found.\n",
      "  Page 4: 0 courses found.\n",
      "  Page 5: 0 courses found.\n",
      "\n",
      "üìö Scraping Subject: Data Analysis\n",
      "  Page 1: 0 courses found.\n",
      "  Page 2: 0 courses found.\n",
      "  Page 3: 0 courses found.\n",
      "  Page 4: 0 courses found.\n",
      "  Page 5: 0 courses found.\n",
      "\n",
      "üìö Scraping Subject: Data Engineering\n",
      "  Page 1: 0 courses found.\n",
      "  Page 2: 0 courses found.\n",
      "  Page 3: 0 courses found.\n",
      "  Page 4: 0 courses found.\n",
      "  Page 5: 0 courses found.\n",
      "\n",
      "üìö Scraping Subject: Computer Science\n",
      "  Page 1: 0 courses found.\n",
      "  Page 2: 0 courses found.\n",
      "  Page 3: 0 courses found.\n",
      "  Page 4: 0 courses found.\n",
      "  Page 5: 0 courses found.\n",
      "\n",
      "‚úÖ Class Central scraping done. Saved to cleaned_classentral.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    driver = setup_driver()\n",
    "\n",
    "    # Jobstreet Scraping\n",
    "    job_links = []\n",
    "    job_links += scrape_jobstreet(\"jobstreet.com.my\", \"malaysia\", driver)\n",
    "    job_links += scrape_jobstreet(\"jobstreet.com.sg\", \"singapore\", driver)\n",
    "    job_links += scrape_jobstreet(\"id.jobstreet.com\", \"indonesia\", driver)\n",
    "\n",
    "    df_jobstreet = scrape_jobstreet_details(job_links, driver)\n",
    "    df_jobstreet.to_csv(\"cleaned_jobstreet.csv\", index=False)\n",
    "    print(\"\\n‚úÖ JobStreet scraping done. Saved to cleaned_jobstreet.csv\")\n",
    "\n",
    "    # Class Central Scraping\n",
    "    urls = {\n",
    "        \"Data Science\": \"https://www.classcentral.com/subject/data-science\",\n",
    "        \"Machine Learning\": \"https://www.classcentral.com/subject/machine-learning\",\n",
    "        \"Data Analysis\": \"https://www.classcentral.com/subject/data-analysis\",\n",
    "        \"Data Engineering\": \"https://www.classcentral.com/subject/data-engineering\",\n",
    "        \"Computer Science\": \"https://www.classcentral.com/subject/cs\"\n",
    "    }\n",
    "\n",
    "    df_classcentral = scrape_classcentral(driver, urls)\n",
    "    df_classcentral.drop_duplicates(subset=[\"Title\", \"Provider\"], inplace=True)\n",
    "    df_classcentral.to_csv(\"cleaned_classentral.csv\", index=False)\n",
    "    print(\"\\n‚úÖ Class Central scraping done. Saved to cleaned_classentral.csv\")\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "459e79d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\dataset\\\\classcentral_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37892\\2299564595.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\\dataset\\\\classcentral_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_job\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hp\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\dataset\\\\classcentral_data.csv'"
     ]
    }
   ],
   "source": [
    "df_job = pd.read_csv('\\\\dataset\\\\classcentral_data.csv')\n",
    "df_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b050b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e41ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library Tambahan\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download resource NLTK untuk Bahasa Inggris\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "print(\"Library berhasil di-import.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus URL, Hashtag, Emoji, Angka, dan Tanda Baca\n",
    "def clean_noise(text):\n",
    "\n",
    "  # Menghapus semua tag HTML secara utuh\n",
    "  text = re.sub(r'<.*?>', '', text)\n",
    "  # Menghapus URL\n",
    "  text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "  # Menghapus Hashtag\n",
    "  text = re.sub(r'#\\w+', '', text)\n",
    "  # Menghapus Emoji dan Tanda Baca\n",
    "  text = re.sub(r'[^\\w\\s]', '', text)\n",
    "  # Menghapus Angka\n",
    "  text = re.sub(r'\\d+', '', text)\n",
    "  # Menghapus spasi berlebih\n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4538a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus Stopwords\n",
    "\n",
    "# Define list_stopwords\n",
    "from nltk.corpus import stopwords\n",
    "list_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\n",
    "  # Memecah kalimat menjadi kata-kata (tokenization)\n",
    "  tokens = text.split()\n",
    "\n",
    "  # Menghapus stopwords dari daftar token\n",
    "  tokens_without_stopwords = [word for word in tokens if word not in list_stopwords]\n",
    "\n",
    "  # Menggabungkan kembali token menjadi kalimat\n",
    "  text = ' '.join(tokens_without_stopwords)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb7700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus URL, Hashtag, Emoji, Angka, dan Tanda Baca\n",
    "def clean_noise(text):\n",
    "\n",
    "  # Menghapus semua tag HTML secara utuh\n",
    "  text = re.sub(r'<.*?>', '', text)\n",
    "  # Menghapus URL\n",
    "  text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "  # Menghapus Hashtag\n",
    "  text = re.sub(r'#\\w+', '', text)\n",
    "  # Menghapus Emoji dan Tanda Baca\n",
    "  text = re.sub(r'[^\\w\\s]', '', text)\n",
    "  # Menghapus Angka\n",
    "  text = re.sub(r'\\d+', '', text)\n",
    "  # Menghapus spasi berlebih\n",
    "  text = re.sub(r'\\s+', ' ', text).strip()\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a7cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus Stopwords\n",
    "\n",
    "# Define list_stopwords\n",
    "from nltk.corpus import stopwords\n",
    "list_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "\n",
    "  # Memecah kalimat menjadi kata-kata (tokenization)\n",
    "  tokens = text.split()\n",
    "\n",
    "  # Menghapus stopwords dari daftar token\n",
    "  tokens_without_stopwords = [word for word in tokens if word not in list_stopwords]\n",
    "\n",
    "  # Menggabungkan kembali token menjadi kalimat\n",
    "  text = ' '.join(tokens_without_stopwords)\n",
    "  return text\n",
    "\n",
    "# Stemming\n",
    "# Membuat stemmer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b05328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_pipeline(text):\n",
    "    text = clean_noise(text)\n",
    "\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # 3. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # 4. Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in list_stopwords]\n",
    "\n",
    "    # 5. Stemming\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    # 6. Join back\n",
    "    cleaned_text = ' '.join(stemmed_tokens)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a12d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# POST-SCRAPING PIPELINE\n",
    "# =====================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_jobs = pd.read_csv(\"/content/jobstreet_data.csv\")\n",
    "\n",
    "if not df_jobs.empty:\n",
    "    print(\"\\n‚úÖ Scraping berhasil. Memulai pipeline post-processing...\")\n",
    "\n",
    "    # Kolom-kolom yang ingin dibersihkan\n",
    "    text_columns = ['Title', 'Company', 'Country', 'Location', 'Category',\n",
    "                    'Work Type', 'Salary', 'Requirements', 'Description']\n",
    "\n",
    "    # Simpan contoh data sebelum preprocessing\n",
    "    sample_row = df_jobs.iloc[0]\n",
    "\n",
    "    # Bersihkan semua kolom teks\n",
    "    for col in text_columns:\n",
    "        df_jobs[f'Cleaned {col}'] = df_jobs[col].apply(cleaning_pipeline)\n",
    "\n",
    "    print(\"\\n--- CONTOH HASIL CLEANING (5 Data Pertama) ---\")\n",
    "    print(df_jobs[[f'Cleaned {col}' for col in text_columns]].head(5))\n",
    "\n",
    "    # Urutkan kolom biar rapi\n",
    "    final_columns = text_columns + [f'Cleaned {col}' for col in text_columns] + ['Link']\n",
    "    df_jobs = df_jobs[final_columns]\n",
    "\n",
    "    # Simpan ke file\n",
    "    output_csv = \"jobstreet_final_data.csv\"\n",
    "    df_jobs.to_csv(output_csv, index=False)\n",
    "    print(f\"\\nüìÅ Data berhasil disimpan ke: '{output_csv}'\")\n",
    "\n",
    "    output_json = \"jobstreet_final_data.json\"\n",
    "    df_jobs.to_json(output_json, orient='records', indent=4)\n",
    "    print(f\"üìÅ Data berhasil disimpan ke: '{output_json}'\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Tidak ada data yang berhasil di-scrape.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
